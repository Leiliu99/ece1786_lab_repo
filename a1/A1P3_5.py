# -*- coding: utf-8 -*-
"""A1_Section3_starter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1avuJcxRTlJg4A_PjSLntmANhB5nJ2yEX
"""

from collections import Counter
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import spacy
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# prepare text using the spacy english pipeline (see https://spacy.io/models/en)
# we'll use it to lemmatize the text, and determine which part of speech each
# lemmatize edits words to become the 'root' word - e.g. holds -> hold;  rubs->rub
# part of speech indicates if the item is a verb, nooun, punctuation, space and so on.
# make sure that the text sent to spacy doesn't end with a period immediately followed by a newline,
# instead, make sure there is a space between the period and the newline, so that the period
# is correctly identified as punctuation.

def prepare_texts(text):
    # Get a callable object from spaCy that processes the text - lemmatizes and determines part of speech

    nlp = spacy.load("en_core_web_sm")

    # lemmatize the text, get part of speech, and remove spaces and punctuation

    lemmas = [tok.lemma_ for tok in nlp(text) if tok.pos_ not in ["PUNCT", "SPACE"]]

    # count the number of occurences of each word in the vocabulary

    freqs = Counter()
    for w in lemmas:
        freqs[w] += 1

    vocab = list(freqs.items())  # List of (word, occurrence)

    vocab = sorted(vocab, key=lambda item: item[1], reverse=True)  # Sort by decreasing frequency

    # Create word->index dictionary and index->word dictionary

    v2i = {v[0]:i for i,v in enumerate(vocab)}
    i2v = {i:v[0] for i,v in enumerate(vocab)}

    return lemmas, v2i, i2v

"""#### This following function walks through each word, and looks at a window (of size 'window') of words and creates input/output prediction pairs, predicting each of the words surrounding the current word from the current word.  So here we say that we are 'predicting the context' from the word"""

def tokenize_and_preprocess_text(textlist, v2i, window):

    # Predict context with word. Sample the context within a window size.
    X, Y = [], []  # is the list of training/test samples
    # TO DO - create all the X,Y pairs

    # store all target-context tuples in list for display purpose
    tuples = []

    # We should limit the window within the sentence
    # So we process within the sentence by spliting the period
    sentences = textlist.split(".")
    nlp = spacy.load("en_core_web_sm")
    for single_sen in sentences:
        # re-lemmatize the text, within this single sentence and remove spaces
        lemmas = [tok.lemma_ for tok in nlp(single_sen) if tok.pos_ not in ["SPACE"]]
        length = len(lemmas)
        for i, word in enumerate(lemmas):
          # we try to find context words
          # from (i-window/2) to (i+window/2), need to skip position i itself
          for j in range(i-window//2, i+window//2+1):
            # skip if out of boundary
            if j < 0 or j >= length:
              continue
            # skip if j=i
            if j == i:
              continue
            # variable word is: target word
            X.append(v2i[word])
            #add context words in Y
            Y.append(v2i[lemmas[j]])

            #add to tuples
            tuples.append((word, lemmas[j]))

    return X, Y, tuples

"""## Define Model that will be trained to produce word vectors"""

class Word2vecModel(torch.nn.Module):
    def __init__(self, vocab_size, embedding_size):
        super().__init__()
        # initialize word vectors to random numbers
        self.embeddings = nn.Embedding(vocab_size, embedding_size)

        # prediction function takes embedding as input, and predicts which word in vocabulary as output
        self.linear = nn.Linear(embedding_size, vocab_size)

    def forward(self, x):
        """
        x: torch.tensor of shape (bsz), bsz is the batch size
        """
        e = self.embeddings(x)
        logits = self.linear(e)
        return logits, e

"""#### The training function - give it the text and it does the rest"""

# custom dataset class for dataloader
class MyDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]


def train_word2vec(textlist, window, v2i, embedding_size ):
    # Set up a model with Skip-gram (predict context with word)
    # textlist: a list of the strings

    # Preprocess the data
    # Create the training data
    data, labels, _ = tokenize_and_preprocess_text(textlist, v2i, window)

    # Split the training data
    data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2, random_state=42)

    # instantiate the network & set up the optimizer
    vocab_size = len(v2i)
    batch_size = 4

    network = Word2vecModel(vocab_size, embedding_size)
    loss_function = nn.CrossEntropyLoss()
    optimizer = optim.Adam(network.parameters(), lr=0.001)

    #create dataloader with batch size and enable shuffling for train
    train_dataset = MyDataset(data_train, labels_train)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    test_dataset = MyDataset(data_test, labels_test)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)


    # training loop
    epochs = 50
    total_trainloss = []
    total_valloss = []
    for epoch in range(epochs):
        epoch_loss = 0
        for target,context in train_loader:
            optimizer.zero_grad()
            output, _ = network(target)
            loss = loss_function(output, context)
            # backpropagate and update the weights
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        print(f'Epoch {epoch+1}, Train Loss: {epoch_loss/len(train_loader)}')
        total_trainloss.append(epoch_loss/len(train_loader))

        # validation
        network.eval()
        val_loss = 0
        with torch.no_grad():
            for target,context in test_loader:
                output, _ = network(target)
                loss = loss_function(output, context)

                val_loss += loss.item()
        print(f'Epoch {epoch+1}, Validation Loss: {val_loss/len(test_loader)}')
        total_valloss.append(val_loss/len(test_loader))

    plt.plot(total_trainloss, label = "Training loss")
    plt.plot(total_valloss, label = "Validation loss")
    plt.title("loss vs. Epoch")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()

    return network

"""#### Evaluate some properties of the word embedding"""

def visualize_embedding(embedding, i2v, most_frequent_from=0, most_frequent_to=40):
    assert embedding.shape[1] == 2, "This only supports visualizing 2-d embeddings!"

    plt.figure()
    for i in range(most_frequent_to):
      x,y = embedding[i,0], embedding[i,1]
      plt.scatter(x,y)
      plt.text(x+0.03, y+0.03, i2v[i])

    plt.title("2-dimensional plot for embeddings")
    plt.xlabel("dimension 0")
    plt.ylabel("dimension 1")
    plt.grid(True)
    plt.show()

if __name__ == "__main__":
    np.random.seed(43)
    torch.manual_seed(43)
    with open('/content/SmallSimpleCorpus.txt','r', encoding='UTF-8') as f:
        txt = f.read()
    _, v2i, i2v = prepare_texts(txt)
    network = train_word2vec(txt, 5, v2i, 2)
    embedding_matrix = network.embeddings.weight.detach().numpy()
    print(embedding_matrix)
    visualize_embedding(embedding_matrix, i2v, 0, 11)